{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_sm\n",
    "from spacy.matcher import Matcher\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition Task\n",
    "## Data Transformation and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the Doc extension \"title\" (default None)\n",
    "Doc.set_extension(\"title\", default=None)\n",
    "\n",
    "# Register the Doc extension \"gddid\" (default None)\n",
    "Doc.set_extension(\"gddid\", default=None)\n",
    "\n",
    "# Register the Doc extension \"sentid\" (default None)\n",
    "Doc.set_extension(\"sentid\", default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.title = str(context[\"title\"])\n",
    "    doc._.gddid = str(context[\"gddid\"])\n",
    "    doc._.sentid = int(context[\"sentid\"])\n",
    "\n",
    "    # Print the text and custom attribute data\n",
    "    # print(f\"{doc._.sentid} - {doc.text}\\n — 'TITLE': {doc._.title}' GDDID': {doc._.gddid}\\n\")\n",
    "    #for ent in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    #    if ent.label_ == \"LOC\": # change to location\n",
    "    #        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD Paterns\n",
    "#dd_pattern = [{\"LOWER\": {\"REGEX\": \"([-]?\\d{1,3}\\.\\d{1,}[,]?[NESWnesw])\"}}, {\"TEXT\":\",\"}, {\"LOWER\": {\"REGEX\": \"([-]?\\d{1,3}\\.\\d{1,}[,]?[NESWnesw])\"}}]\n",
    "dd_pattern2 = [{'LIKE_NUM': True}, {\"TEXT\": {\"REGEX\" : \"[°|o|◦|′|\\'|`|\\\"]\"}}, {\"LOWER\": {\"REGEX\": \"^[nesw]$\"}}, {\"TEXT\":\",\"},{'LIKE_NUM': True}, {\"TEXT\": {\"REGEX\" : \"[°|′|o|\\'|,]\"}}, {\"LOWER\": {\"REGEX\": \"^[nesw]$\"}}]\n",
    "\n",
    "# DMS Patterns\n",
    "dms_pattern = [{'LIKE_NUM': True}, {\"TEXT\": {\"REGEX\" : \"[°|o|◦|′|\\'|`|\\\"]\"}}, {'LIKE_NUM': True}, {\"TEXT\": {\"REGEX\" : \"[°|o|◦|′|\\'|`|\\\"]\"}}, {'LIKE_NUM': True}, {\"TEXT\": {\"REGEX\" : \"[°|o|◦|′|\\'|`|\\\"]\"}}]\n",
    "dms_pattern2 = [{'LIKE_NUM': True}, {\"TEXT\": {\"REGEX\" : \"[°|o|◦|′|\\'|`|\\\"]\"}}, {'LIKE_NUM': True}, {\"TEXT\": {\"REGEX\" : \"[°|o|◦|′|\\'|`|\\\"]\"}}, {\"LOWER\": {\"REGEX\": \"([-]?\\d{1,3}?[nesw])\"}}]\n",
    "\n",
    "# textual patterns\n",
    "#text_pattern = [{'LIKE_NUM': True}, {\"TEXT\": {\"REGEX\" : \"[°|o|◦|′|\\'|`|\\\"]\"}}, {\"LOWER\": {\"REGEX\": \"^[nesw]$\"}}, {\"TEXT\": \",\"}, {'LIKE_NUM': True}, {\"LOWER\": \"minutes\"}, {\"TEXT\": \",\"}, {\"LOWER\":\"and\"},{'LIKE_NUM': True}, {\"LOWER\": \"seconds\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that coordinates do not overlap\n",
    "strategies = [dd_pattern2, dms_pattern, dms_pattern2, text_pattern]\n",
    "entities = []\n",
    "for strategy in strategies:\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"COORDS\", None, strategy)\n",
    "    for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "        matches = matcher(doc)\n",
    "        # Match on the doc and create a list of matched spans\n",
    "        if len(matches) > 0:\n",
    "            spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "            # Initialize auxiliary lists\n",
    "\n",
    "            # Get (start character, end character, label) tuples of matches\n",
    "            coord_entities = [(span.start_char, span.end_char, \"COORDS\") for span in spans]      \n",
    "            new_list = []\n",
    "\n",
    "            for idx, item in enumerate(coord_entities):\n",
    "                #print(\"element\", item)\n",
    "                if new_list == []:\n",
    "                    new_list.append(item)\n",
    "                else:\n",
    "                    aux_list = []\n",
    "                    for idx2, item2 in enumerate(new_list):\n",
    "                        if item[0] in range(item2[0], item2[1]+1):\n",
    "                            aux_list.append(item)\n",
    "                        if item[1] in range(item2[0], item2[1]+1):\n",
    "                            aux_list.append(item)\n",
    "\n",
    "                    if item not in aux_list:\n",
    "                        new_list.append(item)\n",
    "            entities += new_list\n",
    "            print(entities)\n",
    "            \n",
    "        #sym_diff_list = list(set(intersection_list_a).symmetric_difference(set(coord_entities)))\n",
    "\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dd_pattern2\n",
      "[5227 ' N, 5626 ' W]\n",
      "[34 ° S, 41 ° S]\n",
      "[54 ° S, 41 ° S]\n",
      "[4 ' S, 6907 ' W]\n",
      "[1617 ° S, 68.570 ° W]\n",
      "[20 ° S, 68 ° W]\n",
      "[20 ° S, 68 ° W]\n",
      "[3748 ° N, 8067 ° W]\n",
      "[42.50 ° S, 71.40 ° W, 42.20 ° S, 71.17 ° W]\n",
      "dms_pattern\n",
      "[8 Topsail 9 Banook 44 ◦, 23 Long 24 Loon 44 ◦, 29 Power 30 Rocky 44 ◦]\n",
      "[114 ° 110 ° 106 °, 110 ° 106 ° 102 °, 106 ° 102 ° 98 °]\n",
      "[1 Tooth 1 Tooth 1 Tooth, 1 Tooth 1 Tooth 2 Postcranial]\n",
      "[35 ' 30 ' 25 ']\n",
      "[0 Group 1 Group 2 Group]\n",
      "[10 Group 1 Group 2 Group3]\n",
      "[4 ° 30 ' 5 °, 30 ' 5 ° 10 `, 73 ° 4574 ° 25 `]\n",
      "[60 ° 3063 ° 30 `]\n",
      "[74 ° 72 ° 70 °, 72 ° 70 ° 68 °, 70 ° 68 ° 66 °]\n",
      "[35 ° 15 ′ 5135 °, 15 ′ 5135 ° 15 ′]\n",
      "[106 ° 18 ′ 28106 °, 18 ′ 28106 ° 18 ′]\n",
      "[1 Soil 2 Soil 3 Soil, 2 Soil 3 Soil 4 Soil]\n",
      "[0.5 ' 0.6 ` 0.7 ']\n",
      "[150 Kilometers 92 ° 0 ']\n",
      "[30 ° 56.20 ′ 31 °, 56.20 ′ 31 ° 08.54 ′, 31 ° 08.54 ′ 31 °, 08.54 ′ 31 ° 08.54 ′, 31 ° 08.54 ′ 31 °, 08.54 ′ 31 ° 03.50 ′, 31 ° 03.50 ′ 31 °, 03.50 ′ 31 ° 08.54 ′, 31 ° 08.54 ′ 31 °, 08.54 ′ 31 ° 08.43 ′, 31 ° 08.43 ′ 31 °, 08.43 ′ 31 ° 03.53 ′, 31 ° 03.53 ′ 30 °, 03.53 ′ 30 ° 56.49 ′, 30 ° 56.49 ′ 30 °, 56.49 ′ 30 ° 56.37 ′, 30 ° 56.37 ′ 31 °, 56.37 ′ 31 ° 08.56 ′, 31 ° 08.56 ′ 31 °, 08.56 ′ 31 ° 08.53 ′, 31 ° 08.53 ′ 31 °, 08.53 ′ 31 ° 08.53 ′, 31 ° 08.53 ′ 31 °, 08.53 ′ 31 ° 03.53 ′, 31 ° 03.53 ′ 31 °, 03.53 ′ 31 ° 08.44 ′, 31 ° 08.44 ′ 30 °, 08.44 ′ 30 ° 56.23 ′, 30 ° 56.23 ′ 31 °, 56.23 ′ 31 ° 08.54 ′, 31 ° 08.54 ′ 30 °, 08.54 ′ 30 ° 56.27 ′, 30 ° 56.27 ′ 30 °, 56.27 ′ 30 ° 56.19 ′, 30 ° 56.19 ′ 30 °, 56.19 ′ 30 ° 56.06 ′, 30 ° 56.06 ′ 30 °, 56.06 ′ 30 ° 56.27 ′, 30 ° 56.27 ′ 30 °, 56.27 ′ 30 ° 56.23 ′, 30 ° 56.23 ′ 31 °, 56.23 ′ 31 ° 08.67 ′, 31 ° 08.67 ′ 30 °, 08.67 ′ 30 ° 56.27 ′, 30 ° 56.27 ′ 30 °, 56.27 ′ 30 ° 56.27 ′, 30 ° 56.27 ′ 30 °, 56.27 ′ 30 ° 56.27 ′, 30 ° 56.27 ′ 30 °, 56.27 ′ 30 ° 56.12 ′, 30 ° 56.12 ′ 30 °, 56.12 ′ 30 ° 56.12 ′, 30 ° 56.12 ′ 31 °, 56.12 ′ 31 ° 03.50 ′, 31 ° 03.50 ′ 30 °, 03.50 ′ 30 ° 56.60 ′, 30 ° 56.60 ′ 30 °, 56.60 ′ 30 ° 56.35 ′, 30 ° 56.35 ′ 30 °, 56.35 ′ 30 ° 56.51 ′, 30 ° 56.51 ′ 30 °, 56.51 ′ 30 ° 56.57 ′, 30 ° 56.57 ′ 30 °, 56.57 ′ 30 ° 56.60 ′, 30 ° 56.60 ′ 30 °, 56.60 ′ 30 ° 56.51 ′, 30 ° 56.51 ′ 31 °, 56.51 ′ 31 ° 08.54 ′, 31 ° 08.54 ′ 30 °, 08.54 ′ 30 ° 56.49 ′, 30 ° 56.49 ′ 30 °, 56.49 ′ 30 ° 56.49 ′, 30 ° 56.49 ′ 30 °, 56.49 ′ 30 ° 56.49 ′, 30 ° 56.49 ′ 115 °, 56.49 ′ 115 ° 15.53 ′, 115 ° 15.53 ′ 115 °, 15.53 ′ 115 ° 25.57 ′, 115 ° 25.57 ′ 115 °, 25.57 ′ 115 ° 25.65 ′, 115 ° 25.65 ′ 115 °, 25.65 ′ 115 ° 21.39 ′, 115 ° 21.39 ′ 115 °, 21.39 ′ 115 ° 25.65 ′, 115 ° 25.65 ′ 115 °, 25.65 ′ 115 ° 25.60 ′, 115 ° 25.60 ′ 115 °, 25.60 ′ 115 ° 21.40 ′, 115 ° 21.40 ′ 115 °, 21.40 ′ 115 ° 15.45 ′, 115 ° 15.45 ′ 115 °, 15.45 ′ 115 ° 15.60 ′, 115 ° 15.60 ′ 115 °, 15.60 ′ 115 ° 25.73 ′, 115 ° 25.73 ′ 115 °, 25.73 ′ 115 ° 25.70 ′, 115 ° 25.70 ′ 115 °, 25.70 ′ 115 ° 25.70 ′, 115 ° 25.70 ′ 115 °, 25.70 ′ 115 ° 21.44 ′, 115 ° 21.44 ′ 115 °, 21.44 ′ 115 ° 25.62 ′, 115 ° 25.62 ′ 115 °, 25.62 ′ 115 ° 15.28 ′, 115 ° 15.28 ′ 115 °, 15.28 ′ 115 ° 25.69 ′, 115 ° 25.69 ′ 115 °, 25.69 ′ 115 ° 15.30 ′, 115 ° 15.30 ′ 115 °, 15.30 ′ 115 ° 15.33 ′, 115 ° 15.33 ′ 115 °, 15.33 ′ 115 ° 15.18 ′, 115 ° 15.18 ′ 115 °, 15.18 ′ 115 ° 15.25 ′, 115 ° 15.25 ′ 115 °, 15.25 ′ 115 ° 15.28 ′, 115 ° 15.28 ′ 115 °, 15.28 ′ 115 ° 25.67 ′, 115 ° 25.67 ′ 115 °, 25.67 ′ 115 ° 15.32 ′, 115 ° 15.32 ′ 115 °, 15.32 ′ 115 ° 15.32 ′, 115 ° 15.32 ′ 115 °, 15.32 ′ 115 ° 15.30 ′, 115 ° 15.30 ′ 115 °, 15.30 ′ 115 ° 15.13 ′, 115 ° 15.13 ′ 115 °, 15.13 ′ 115 ° 15.13 ′, 115 ° 15.13 ′ 115 °, 15.13 ′ 115 ° 21.41 ′, 115 ° 21.41 ′ 115 °, 21.41 ′ 115 ° 15.64 ′, 115 ° 15.64 ′ 115 °, 15.64 ′ 115 ° 15.61 ′, 115 ° 15.61 ′ 115 °, 15.61 ′ 115 ° 15.74 ′, 115 ° 15.74 ′ 115 °, 15.74 ′ 115 ° 15.16 ′, 115 ° 15.16 ′ 115 °, 15.16 ′ 115 ° 15.64 ′, 115 ° 15.64 ′ 115 °, 15.64 ′ 115 ° 15.50 ′, 115 ° 15.50 ′ 115 °, 15.50 ′ 115 ° 25.69 ′, 115 ° 25.69 ′ 115 °, 25.69 ′ 115 ° 15.45 ′, 115 ° 15.45 ′ 115 °, 15.45 ′ 115 ° 15.45 ′, 115 ° 15.45 ′ 115 °, 15.45 ′ 115 ° 15.45 ′]\n",
      "[1600 Contraction 1800 2ooo 2200 WarmMoist__Cool]\n",
      "[1 ° 30 'S 70 °, 30 'S 70 ° 11 ']\n",
      "[0 ° 25 'S 71 °, 25 'S 71 ° 55 ']\n",
      "[0040 ' 00 ' 2000 Radiocarbon]\n",
      "[24 ° 26 ° 28 °]\n",
      "[47 ° 49 ' 23 °, 49 ' 23 ° 31 ']\n",
      "[3.5 Wood 3.5 Wood 3.5 Wood, 3.5 Wood 3.5 Wood 3.6 Wood, 3.5 Wood 3.6 Wood 4.3 Wood]\n",
      "[49 cocped 50 chasoes 51 nitscfon, 50 chasoes 51 nitscfon 52 navlibo, 51 nitscfon 52 navlibo 53 cycoce, 88 navmod 89 braneo 1972 Taxon]\n",
      "[134 staneo 135 stapho 136 dipnsjo, 135 stapho 136 dipnsjo 137 navdigo]\n",
      "dms_pattern2\n",
      "[15 ° 57 ' 32.9E]\n",
      "[69 ° 7 ' 30W]\n",
      "[92 ° 0 ' 0W, 90 ° 0 ' 0W, 88 ° 0 ' 0W, 86 ° 0 ' 0W, 84 ° 0 ' 0W, 24 ° 0 ' 0N, 22 ° 0 ' 0N, 20 ° 0 ' 0N]\n",
      "[92 ° 0 ' 0W, 90 ° 0 ' 0W, 88 ° 0 ' 0W, 86 ° 0 ' 0W]\n",
      "[84 ° 0 ' 0W, 18 ° 0 ' 0N]\n",
      "[45 ° 15 ' 20N, 67 ° 19 ' 50W]\n",
      "[46 ° 45 ' 25N]\n",
      "[46 ° 00 ' 50N, 66 ° 37 ' 40W]\n",
      "[45 ° 30 ' 50N, 62 ° 31 ' 05W]\n",
      "[44 ° 15 ' 50N, 66 ° 04 ' 45W, 44 ° 40 ' 05N, 63 ° 56 ' 20W]\n",
      "[45 ° 39 ' 05N]\n",
      "[45 ' 15 ' 15N.]\n",
      "[156 ° 15 ′ 12W]\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA = []\n",
    "TESTING_DATA = []\n",
    "\n",
    "strategies = {\"dd_pattern2\":dd_pattern2, \"dms_pattern\": dms_pattern, \"dms_pattern2\":dms_pattern2}\n",
    "\n",
    "#strategies = [dd_pattern, dd_pattern2, dd_pattern3, dms_pattern, dms_pattern2, text_pattern]\n",
    "entities = []\n",
    "for strategy, pattern in strategies.items():\n",
    "    print(strategy)\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"COORDS\", None, pattern)\n",
    "\n",
    "    for doc, context in nlp.pipe(data, as_tuples=True):\n",
    "        matches = matcher(doc)\n",
    "        # Match on the doc and create a list of matched spans\n",
    "        if len(matches) > 0:\n",
    "            spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "            print(spans)\n",
    "            # Initialize auxiliary lists\n",
    "            intersection_list = []\n",
    "            # Get (start character, end character, label) tuples of matches\n",
    "            coord_entities = [(span.start_char, span.end_char, \"COORDS\") for span in spans]\n",
    "            \n",
    "\n",
    "            # make sure coords dont overlap\n",
    "            new_list = []\n",
    "            for idx, item in enumerate(coord_entities):\n",
    "                #print(\"element\", item)\n",
    "                if new_list == []:\n",
    "                    new_list.append(item)\n",
    "                else:\n",
    "                    aux_list = []\n",
    "                    for idx2, item2 in enumerate(new_list):\n",
    "                        if item[0] in range(item2[0], item2[1]+1):\n",
    "                            aux_list.append(item)\n",
    "                        if item[1] in range(item2[0], item2[1]+1):\n",
    "                            aux_list.append(item)\n",
    "\n",
    "                    if item not in aux_list:\n",
    "                        new_list.append(item)\n",
    "\n",
    "            coords_copy = new_list.copy() # aux list\n",
    "    \n",
    "            # OLD entities\n",
    "            old_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents if ent.label_ == \"LOC\" or ent.label_ == \"GPE\"]\n",
    "            # All entities\n",
    "            for old_entity in old_entities:\n",
    "                for coords in coords_copy:\n",
    "                    if old_entity[0] in range(coords[0], coords[1]):\n",
    "                        intersection_list.append(old_entity)\n",
    "                    if old_entity[1] in range(coords[0], coords[1]):\n",
    "                        intersection_list.append(old_entity)\n",
    "\n",
    "            sym_diff_list = list(set(intersection_list).symmetric_difference(set(old_entities)))\n",
    "\n",
    "            entities = sym_diff_list + coord_entities\n",
    "            #entities3 = entities + entities2\n",
    "            # Format the matches as a (doc.text, entities) tuple\n",
    "            training_example = (doc.text, {\"entities\": entities})\n",
    "            # Append the example to the training data\n",
    "            TRAINING_DATA.append(training_example)\n",
    "        \n",
    "        else:\n",
    "            old_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents if ent.label_ == \"LOC\" or ent.label_ == \"GPE\"]\n",
    "            testing_example = (doc.text, {\"entities\": entities})\n",
    "            TESTING_DATA.append(testing_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAINING_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testing_data.json', 'w') as f:\n",
    "    json.dump(TESTING_DATA, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(*TRAINING_DATA, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_idx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 16, 21, 22, 28, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a_series = pd.Series(TRAINING_DATA)\n",
    "accessed_series = a_series[select_idx]\n",
    "accessed_list = list(accessed_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - altair\n",
      "    - vega_datasets\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    altair-4.1.0               |             py_1         614 KB  conda-forge\n",
      "    vega_datasets-0.9.0        |     pyhd3deb0d_0         179 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         793 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  altair             conda-forge/noarch::altair-4.1.0-py_1\n",
      "  vega_datasets      conda-forge/noarch::vega_datasets-0.9.0-pyhd3deb0d_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda              pkgs/main::conda-4.10.3-py38hecd8cb5_0 --> conda-forge::conda-4.10.3-py38h50d1736_2\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge altair vega_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
